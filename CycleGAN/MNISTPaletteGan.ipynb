{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "import umap\n",
    "import glob\n",
    "import torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "__all__ = [\"ColorMNIST\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-07 16:22:12--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Resolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n",
      "--2021-05-07 16:22:13--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
      "Connecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-gzip]\n",
      "Saving to: ‘MNIST.tar.gz.1’\n",
      "\n",
      "MNIST.tar.gz.1          [   <=>              ]  33.20M  1.22MB/s    in 1m 47s  \n",
      "\n",
      "2021-05-07 16:24:01 (317 KB/s) - ‘MNIST.tar.gz.1’ saved [34813078]\n",
      "\n",
      "x MNIST/\n",
      "x MNIST/raw/\n",
      "x MNIST/raw/train-labels-idx1-ubyte\n",
      "x MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "x MNIST/raw/t10k-labels-idx1-ubyte\n",
      "x MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "x MNIST/raw/train-images-idx3-ubyte\n",
      "x MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "x MNIST/raw/t10k-images-idx3-ubyte\n",
      "x MNIST/raw/train-images-idx3-ubyte.gz\n",
      "x MNIST/processed/\n",
      "x MNIST/processed/training.pt\n",
      "x MNIST/processed/test.pt\n"
     ]
    }
   ],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "train_data = MNIST(root = '/Users/arpit/Desktop/GAN/MNIST_Dataset/', train=True, download=True, transform=transforms)\n",
    "val_data = MNIST(root = '/Users/arpit/Desktop/GAN/MNIST_Dataset/', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorMNIST(Dataset):\n",
    "    def __init__(self, color, split, path, transform_list=[], randomcolor=False):\n",
    "        assert color in ['num', 'back', 'both'], \"color must be either 'num', 'back' or 'both\"\n",
    "        self.pallette = [[31, 119, 180],\n",
    "                         [255, 127, 14],\n",
    "                         [44, 160, 44],\n",
    "                         [214, 39, 40],\n",
    "                         [148, 103, 189],\n",
    "                         [140, 86, 75],\n",
    "                         [227, 119, 194],\n",
    "                         [127, 127, 127],\n",
    "                         [188, 189, 34],\n",
    "                         [23, 190, 207]]\n",
    "\n",
    "        if split == 'train':\n",
    "            fimages = os.path.join(path, 'raw', 'train-images-idx3-ubyte')\n",
    "            flabels = os.path.join(path, 'raw', 'train-labels-idx1-ubyte')\n",
    "        else:\n",
    "            fimages = os.path.join(path, 'raw', 't10k-images-idx3-ubyte')\n",
    "            flabels = os.path.join(path, 'raw', 't10k-labels-idx1-ubyte')\n",
    "\n",
    "        # Load images\n",
    "        with open(fimages, 'rb') as f:\n",
    "            _, _, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "            self.images = np.fromfile(f, dtype=np.uint8).reshape(-1, rows, cols)\n",
    "\n",
    "        # Load labels\n",
    "        with open(flabels, 'rb') as f:\n",
    "            struct.unpack(\">II\", f.read(8))\n",
    "            self.labels = np.fromfile(f, dtype=np.int8)\n",
    "            self.labels = torch.from_numpy(self.labels.astype(np.int))\n",
    "\n",
    "        self.transform_list = transform_list\n",
    "        self.color = color\n",
    "        self.images = np.tile(self.images[:, :, :, np.newaxis], 3)\n",
    "        self.randomcolor = randomcolor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # Range [0,255]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Choose color\n",
    "        if self.randomcolor:\n",
    "            c = self.pallette[np.random.randint(0, 10)]\n",
    "            if self.color == 'both':\n",
    "                while True:\n",
    "                    c2 = self.pallette[np.random.randint(0, 10)]\n",
    "                    if c2 != c: break\n",
    "        else:\n",
    "            if self.color == 'num':\n",
    "                c = self.pallette[-(label + 1)]\n",
    "            elif self.color == 'back':\n",
    "                c = self.pallette[label]\n",
    "            else:\n",
    "                c = self.pallette[label]\n",
    "                c2 = self.pallette[-(label - 3)]\n",
    "\n",
    "        # Assign color according to their class (0,10)\n",
    "        if self.color == 'num':\n",
    "            image[:, :, 0] = image[:, :, 0] / 255 * c[0]\n",
    "            image[:, :, 1] = image[:, :, 1] / 255 * c[1]\n",
    "            image[:, :, 2] = image[:, :, 2] / 255 * c[2]\n",
    "        elif self.color == 'back':\n",
    "            image[:, :, 0] = (255 - image[:, :, 0]) / 255 * c[0]\n",
    "            image[:, :, 1] = (255 - image[:, :, 1]) / 255 * c[1]\n",
    "            image[:, :, 2] = (255 - image[:, :, 2]) / 255 * c[2]\n",
    "        else:\n",
    "            image[:, :, 0] = image[:, :, 0] / 255 * c[0] + (255 - image[:, :, 0]) / 255 * c2[0]\n",
    "            image[:, :, 1] = image[:, :, 1] / 255 * c[1] + (255 - image[:, :, 1]) / 255 * c2[1]\n",
    "            image[:, :, 2] = image[:, :, 2] / 255 * c[2] + (255 - image[:, :, 2]) / 255 * c2[2]\n",
    "\n",
    "        image = Image.fromarray(image)\n",
    "        for t in self.transform_list:\n",
    "            image = t(image)\n",
    "        image = transforms.ToTensor()(image)  # Range [0,1]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataloader = ColorMNIST('both', 'train', '/Users/arpit/Desktop/GAN/MNIST_Dataset/MNIST/', randomcolor=True)\n",
    "\n",
    "x_all = []\n",
    "for i in [1, 3, 5, 7, 2, 0, 13, 15, 17, 4]:\n",
    "    x_all.append(dataloader[i][0].numpy().transpose([1, 2, 0]))\n",
    "x_all = np.hstack(x_all)\n",
    "\n",
    "plt.imshow(x_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Paintings\n",
    "path = '/Users/arpit/Desktop/GAN/Paintings100/*.jpg'\n",
    "filenames = [img for img in glob.glob(path)]\n",
    "filenames.sort()\n",
    "#Reading Generated Paintings\n",
    "path_1 = '/Users/arpit/Desktop/GAN/GeneratedPaintiings100/*.png'\n",
    "filenames_1 = [img for img in glob.glob(path_1)]\n",
    "filenames_1.sort()\n",
    "# Monet Palette\n",
    "mp_palette = []\n",
    "for name in filenames:\n",
    "    img = cv2.imread(name,cv2.IMREAD_UNCHANGED)\n",
    "    data = img.reshape((-1, 3))\n",
    "    data = np.float32(data)\n",
    "\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    compactness,labels,centers = cv2.kmeans(data,1,None,criteria,10,flags)\n",
    "\n",
    "    #print('Monet Palette Dominant color is: BGR ({})'.format(centers[0].astype(np.int32)))\n",
    "    mp_palette.append(centers[0].astype(np.int32))\n",
    "    \n",
    "#Generated Monet Palette\n",
    "gen_palette = []\n",
    "for name in filenames_1:\n",
    "    img = cv2.imread(name,cv2.IMREAD_UNCHANGED)\n",
    "    data = img.reshape((-1, 3))\n",
    "    data = np.float32(data)\n",
    "\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "    compactness,labels,centers = cv2.kmeans(data,1,None,criteria,10,flags)\n",
    "\n",
    "    #print('Generated Monet Palette Dominant color is: BGR({})'.format(centers[0].astype(np.int32)))\n",
    "    gen_palette.append(centers[0].astype(np.int32))\n",
    "rgb_pal = [i[[2,1,0]] for i in mp_palette]\n",
    "X = [i/255 for i in rgb_pal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=list()\n",
    "for line in rgb_pal:\n",
    "    line = list(line)\n",
    "    newdata.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaletteMNIST(Dataset):\n",
    "    def __init__(self, color, split, path, transform_list=[], randomcolor=False):\n",
    "        assert color in ['num', 'back', 'both'], \"color must be either 'num', 'back' or 'both\"\n",
    "        self.pallette = newdata\n",
    "\n",
    "        if split == 'train':\n",
    "            fimages = os.path.join(path, 'raw', 'train-images-idx3-ubyte')\n",
    "            flabels = os.path.join(path, 'raw', 'train-labels-idx1-ubyte')\n",
    "        else:\n",
    "            fimages = os.path.join(path, 'raw', 't10k-images-idx3-ubyte')\n",
    "            flabels = os.path.join(path, 'raw', 't10k-labels-idx1-ubyte')\n",
    "\n",
    "        # Load images\n",
    "        with open(fimages, 'rb') as f:\n",
    "            _, _, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "            self.images = np.fromfile(f, dtype=np.uint8).reshape(-1, rows, cols)\n",
    "\n",
    "        # Load labels\n",
    "        with open(flabels, 'rb') as f:\n",
    "            struct.unpack(\">II\", f.read(8))\n",
    "            self.labels = np.fromfile(f, dtype=np.int8)\n",
    "            self.labels = torch.from_numpy(self.labels.astype(np.int))\n",
    "\n",
    "        self.transform_list = transform_list\n",
    "        self.color = color\n",
    "        self.images = np.tile(self.images[:, :, :, np.newaxis], 3)\n",
    "        self.randomcolor = randomcolor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # Range [0,255]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Choose color\n",
    "        if self.randomcolor:\n",
    "            c = self.pallette[np.random.randint(0, 10)]\n",
    "            if self.color == 'both':\n",
    "                while True:\n",
    "                    c2 = self.pallette[np.random.randint(0, 10)]\n",
    "                    if c2 != c: break\n",
    "        else:\n",
    "            if self.color == 'num':\n",
    "                c = self.pallette[-(label + 1)]\n",
    "            elif self.color == 'back':\n",
    "                c = self.pallette[label]\n",
    "            else:\n",
    "                c = self.pallette[label]\n",
    "                c2 = self.pallette[-(label - 3)]\n",
    "\n",
    "        # Assign color according to their class (0,10)\n",
    "        if self.color == 'num':\n",
    "            image[:, :, 0] = image[:, :, 0] / 255 * c[0]\n",
    "            image[:, :, 1] = image[:, :, 1] / 255 * c[1]\n",
    "            image[:, :, 2] = image[:, :, 2] / 255 * c[2]\n",
    "        elif self.color == 'back':\n",
    "            image[:, :, 0] = (255 - image[:, :, 0]) / 255 * c[0]\n",
    "            image[:, :, 1] = (255 - image[:, :, 1]) / 255 * c[1]\n",
    "            image[:, :, 2] = (255 - image[:, :, 2]) / 255 * c[2]\n",
    "        else:\n",
    "            image[:, :, 0] = image[:, :, 0] / 255 * c[0] + (255 - image[:, :, 0]) / 255 * c2[0]\n",
    "            image[:, :, 1] = image[:, :, 1] / 255 * c[1] + (255 - image[:, :, 1]) / 255 * c2[1]\n",
    "            image[:, :, 2] = image[:, :, 2] / 255 * c[2] + (255 - image[:, :, 2]) / 255 * c2[2]\n",
    "\n",
    "        image = Image.fromarray(image)\n",
    "        for t in self.transform_list:\n",
    "            image = t(image)\n",
    "        image = transforms.ToTensor()(image)  # Range [0,1]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = PaletteMNIST('both', 'train', '/Users/arpit/Desktop/GAN/MNIST_Dataset/MNIST/', randomcolor=True)\n",
    "\n",
    "x_all = []\n",
    "for i in range(60000):\n",
    "    x_all.append(dataloader[i][0].numpy().transpose([1, 2, 0]))\n",
    "\n",
    "#plt.imshow(x_all)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff948a2e6d0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_all[59999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        ...,\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864]],\n",
       "\n",
       "       [[0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        ...,\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864]],\n",
       "\n",
       "       [[0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        ...,\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        ...,\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864]],\n",
       "\n",
       "       [[0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        ...,\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864]],\n",
       "\n",
       "       [[0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        ...,\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864],\n",
       "        [0.43137255, 0.49803922, 0.32156864]]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_all)):\n",
    "    plt.imsave('ColorMnist/img'+str(i)+'.png', x_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder('MNISTnew', transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "img_size = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(512, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        #print('img',img.shape)\n",
    "        out = self.model(img)\n",
    "        #print(\"1\",out.shape)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        #print(\"2\",out.shape)\n",
    "        validity = self.adv_layer(out)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(100, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (adv_layer): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5,0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5,0.999))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/600] [D loss: 0.693366] [G loss: 0.714394]\n",
      "[Epoch 0/200] [Batch 1/600] [D loss: 0.693379] [G loss: 0.713697]\n",
      "[Epoch 0/200] [Batch 2/600] [D loss: 0.693389] [G loss: 0.713095]\n",
      "[Epoch 0/200] [Batch 3/600] [D loss: 0.693327] [G loss: 0.712382]\n",
      "[Epoch 0/200] [Batch 4/600] [D loss: 0.693316] [G loss: 0.711824]\n",
      "[Epoch 0/200] [Batch 5/600] [D loss: 0.693274] [G loss: 0.711059]\n",
      "[Epoch 0/200] [Batch 6/600] [D loss: 0.693311] [G loss: 0.710503]\n",
      "[Epoch 0/200] [Batch 7/600] [D loss: 0.693303] [G loss: 0.709917]\n",
      "[Epoch 0/200] [Batch 8/600] [D loss: 0.693260] [G loss: 0.709198]\n",
      "[Epoch 0/200] [Batch 9/600] [D loss: 0.693316] [G loss: 0.708562]\n",
      "[Epoch 0/200] [Batch 10/600] [D loss: 0.693288] [G loss: 0.707875]\n",
      "[Epoch 0/200] [Batch 11/600] [D loss: 0.693250] [G loss: 0.707286]\n",
      "[Epoch 0/200] [Batch 12/600] [D loss: 0.693247] [G loss: 0.706525]\n",
      "[Epoch 0/200] [Batch 13/600] [D loss: 0.693294] [G loss: 0.705970]\n",
      "[Epoch 0/200] [Batch 14/600] [D loss: 0.693249] [G loss: 0.705359]\n",
      "[Epoch 0/200] [Batch 15/600] [D loss: 0.693194] [G loss: 0.704692]\n",
      "[Epoch 0/200] [Batch 16/600] [D loss: 0.693240] [G loss: 0.704123]\n",
      "[Epoch 0/200] [Batch 17/600] [D loss: 0.693252] [G loss: 0.703556]\n",
      "[Epoch 0/200] [Batch 18/600] [D loss: 0.693219] [G loss: 0.703018]\n",
      "[Epoch 0/200] [Batch 19/600] [D loss: 0.693209] [G loss: 0.702270]\n",
      "[Epoch 0/200] [Batch 20/600] [D loss: 0.693195] [G loss: 0.701758]\n",
      "[Epoch 0/200] [Batch 21/600] [D loss: 0.693205] [G loss: 0.701287]\n",
      "[Epoch 0/200] [Batch 22/600] [D loss: 0.693165] [G loss: 0.700688]\n",
      "[Epoch 0/200] [Batch 23/600] [D loss: 0.693217] [G loss: 0.700267]\n",
      "[Epoch 0/200] [Batch 24/600] [D loss: 0.693231] [G loss: 0.699741]\n",
      "[Epoch 0/200] [Batch 25/600] [D loss: 0.693182] [G loss: 0.699322]\n",
      "[Epoch 0/200] [Batch 26/600] [D loss: 0.693150] [G loss: 0.698880]\n",
      "[Epoch 0/200] [Batch 27/600] [D loss: 0.693169] [G loss: 0.698520]\n",
      "[Epoch 0/200] [Batch 28/600] [D loss: 0.693142] [G loss: 0.698159]\n",
      "[Epoch 0/200] [Batch 29/600] [D loss: 0.693154] [G loss: 0.697661]\n",
      "[Epoch 0/200] [Batch 30/600] [D loss: 0.693125] [G loss: 0.697200]\n",
      "[Epoch 0/200] [Batch 31/600] [D loss: 0.693177] [G loss: 0.696829]\n",
      "[Epoch 0/200] [Batch 32/600] [D loss: 0.693148] [G loss: 0.696496]\n",
      "[Epoch 0/200] [Batch 33/600] [D loss: 0.693180] [G loss: 0.696265]\n",
      "[Epoch 0/200] [Batch 34/600] [D loss: 0.693116] [G loss: 0.695993]\n",
      "[Epoch 0/200] [Batch 35/600] [D loss: 0.693147] [G loss: 0.695627]\n",
      "[Epoch 0/200] [Batch 36/600] [D loss: 0.693148] [G loss: 0.695379]\n",
      "[Epoch 0/200] [Batch 37/600] [D loss: 0.693156] [G loss: 0.695105]\n",
      "[Epoch 0/200] [Batch 38/600] [D loss: 0.693178] [G loss: 0.694903]\n",
      "[Epoch 0/200] [Batch 39/600] [D loss: 0.693191] [G loss: 0.694757]\n",
      "[Epoch 0/200] [Batch 40/600] [D loss: 0.693188] [G loss: 0.694609]\n",
      "[Epoch 0/200] [Batch 41/600] [D loss: 0.693140] [G loss: 0.694414]\n",
      "[Epoch 0/200] [Batch 42/600] [D loss: 0.693135] [G loss: 0.694239]\n",
      "[Epoch 0/200] [Batch 43/600] [D loss: 0.693201] [G loss: 0.694061]\n",
      "[Epoch 0/200] [Batch 44/600] [D loss: 0.693176] [G loss: 0.693940]\n",
      "[Epoch 0/200] [Batch 45/600] [D loss: 0.693168] [G loss: 0.693922]\n",
      "[Epoch 0/200] [Batch 46/600] [D loss: 0.693204] [G loss: 0.693750]\n",
      "[Epoch 0/200] [Batch 47/600] [D loss: 0.693140] [G loss: 0.693588]\n",
      "[Epoch 0/200] [Batch 48/600] [D loss: 0.693175] [G loss: 0.693642]\n",
      "[Epoch 0/200] [Batch 49/600] [D loss: 0.693195] [G loss: 0.693505]\n",
      "[Epoch 0/200] [Batch 50/600] [D loss: 0.693202] [G loss: 0.693451]\n",
      "[Epoch 0/200] [Batch 51/600] [D loss: 0.693140] [G loss: 0.693347]\n",
      "[Epoch 0/200] [Batch 52/600] [D loss: 0.693076] [G loss: 0.693367]\n",
      "[Epoch 0/200] [Batch 53/600] [D loss: 0.693158] [G loss: 0.693227]\n",
      "[Epoch 0/200] [Batch 54/600] [D loss: 0.693177] [G loss: 0.693309]\n",
      "[Epoch 0/200] [Batch 55/600] [D loss: 0.693234] [G loss: 0.693298]\n",
      "[Epoch 0/200] [Batch 56/600] [D loss: 0.693128] [G loss: 0.693341]\n",
      "[Epoch 0/200] [Batch 57/600] [D loss: 0.693164] [G loss: 0.693371]\n",
      "[Epoch 0/200] [Batch 58/600] [D loss: 0.693054] [G loss: 0.693282]\n",
      "[Epoch 0/200] [Batch 59/600] [D loss: 0.693142] [G loss: 0.693281]\n",
      "[Epoch 0/200] [Batch 60/600] [D loss: 0.693101] [G loss: 0.693425]\n",
      "[Epoch 0/200] [Batch 61/600] [D loss: 0.693125] [G loss: 0.693329]\n",
      "[Epoch 0/200] [Batch 62/600] [D loss: 0.693093] [G loss: 0.693389]\n",
      "[Epoch 0/200] [Batch 63/600] [D loss: 0.693107] [G loss: 0.693411]\n",
      "[Epoch 0/200] [Batch 64/600] [D loss: 0.693103] [G loss: 0.693413]\n",
      "[Epoch 0/200] [Batch 65/600] [D loss: 0.693137] [G loss: 0.693310]\n",
      "[Epoch 0/200] [Batch 66/600] [D loss: 0.693101] [G loss: 0.693311]\n",
      "[Epoch 0/200] [Batch 67/600] [D loss: 0.693238] [G loss: 0.693216]\n",
      "[Epoch 0/200] [Batch 68/600] [D loss: 0.693211] [G loss: 0.693135]\n",
      "[Epoch 0/200] [Batch 69/600] [D loss: 0.693270] [G loss: 0.692960]\n",
      "[Epoch 0/200] [Batch 70/600] [D loss: 0.693269] [G loss: 0.692953]\n",
      "[Epoch 0/200] [Batch 71/600] [D loss: 0.693159] [G loss: 0.692973]\n",
      "[Epoch 0/200] [Batch 72/600] [D loss: 0.693142] [G loss: 0.693103]\n",
      "[Epoch 0/200] [Batch 73/600] [D loss: 0.693216] [G loss: 0.693156]\n",
      "[Epoch 0/200] [Batch 74/600] [D loss: 0.693148] [G loss: 0.693081]\n",
      "[Epoch 0/200] [Batch 75/600] [D loss: 0.693158] [G loss: 0.693160]\n",
      "[Epoch 0/200] [Batch 76/600] [D loss: 0.693174] [G loss: 0.693113]\n",
      "[Epoch 0/200] [Batch 77/600] [D loss: 0.693196] [G loss: 0.693011]\n",
      "[Epoch 0/200] [Batch 78/600] [D loss: 0.693154] [G loss: 0.693142]\n",
      "[Epoch 0/200] [Batch 79/600] [D loss: 0.693155] [G loss: 0.693127]\n",
      "[Epoch 0/200] [Batch 80/600] [D loss: 0.693210] [G loss: 0.693087]\n",
      "[Epoch 0/200] [Batch 81/600] [D loss: 0.693131] [G loss: 0.693121]\n",
      "[Epoch 0/200] [Batch 82/600] [D loss: 0.693165] [G loss: 0.693196]\n",
      "[Epoch 0/200] [Batch 83/600] [D loss: 0.693137] [G loss: 0.693115]\n",
      "[Epoch 0/200] [Batch 84/600] [D loss: 0.693012] [G loss: 0.693306]\n",
      "[Epoch 0/200] [Batch 85/600] [D loss: 0.693043] [G loss: 0.693238]\n",
      "[Epoch 0/200] [Batch 86/600] [D loss: 0.693104] [G loss: 0.693138]\n",
      "[Epoch 0/200] [Batch 87/600] [D loss: 0.693073] [G loss: 0.693121]\n",
      "[Epoch 0/200] [Batch 88/600] [D loss: 0.693205] [G loss: 0.693104]\n",
      "[Epoch 0/200] [Batch 89/600] [D loss: 0.693137] [G loss: 0.692908]\n",
      "[Epoch 0/200] [Batch 90/600] [D loss: 0.693195] [G loss: 0.693017]\n",
      "[Epoch 0/200] [Batch 91/600] [D loss: 0.693207] [G loss: 0.692902]\n",
      "[Epoch 0/200] [Batch 92/600] [D loss: 0.693110] [G loss: 0.693050]\n",
      "[Epoch 0/200] [Batch 93/600] [D loss: 0.693035] [G loss: 0.693129]\n",
      "[Epoch 0/200] [Batch 94/600] [D loss: 0.692899] [G loss: 0.693297]\n",
      "[Epoch 0/200] [Batch 95/600] [D loss: 0.692879] [G loss: 0.693335]\n",
      "[Epoch 0/200] [Batch 96/600] [D loss: 0.692919] [G loss: 0.693340]\n",
      "[Epoch 0/200] [Batch 97/600] [D loss: 0.693144] [G loss: 0.693131]\n",
      "[Epoch 0/200] [Batch 98/600] [D loss: 0.693304] [G loss: 0.692737]\n",
      "[Epoch 0/200] [Batch 99/600] [D loss: 0.693586] [G loss: 0.692205]\n",
      "[Epoch 0/200] [Batch 100/600] [D loss: 0.693640] [G loss: 0.692091]\n",
      "[Epoch 0/200] [Batch 101/600] [D loss: 0.693480] [G loss: 0.692589]\n",
      "[Epoch 0/200] [Batch 102/600] [D loss: 0.693555] [G loss: 0.692481]\n",
      "[Epoch 0/200] [Batch 103/600] [D loss: 0.693410] [G loss: 0.692987]\n",
      "[Epoch 0/200] [Batch 104/600] [D loss: 0.693260] [G loss: 0.693736]\n",
      "[Epoch 0/200] [Batch 105/600] [D loss: 0.692534] [G loss: 0.694798]\n",
      "[Epoch 0/200] [Batch 106/600] [D loss: 0.692352] [G loss: 0.695066]\n",
      "[Epoch 0/200] [Batch 107/600] [D loss: 0.692023] [G loss: 0.695903]\n",
      "[Epoch 0/200] [Batch 108/600] [D loss: 0.691863] [G loss: 0.696318]\n",
      "[Epoch 0/200] [Batch 109/600] [D loss: 0.691887] [G loss: 0.696449]\n",
      "[Epoch 0/200] [Batch 110/600] [D loss: 0.691978] [G loss: 0.696225]\n",
      "[Epoch 0/200] [Batch 111/600] [D loss: 0.692633] [G loss: 0.695171]\n",
      "[Epoch 0/200] [Batch 112/600] [D loss: 0.694127] [G loss: 0.692189]\n",
      "[Epoch 0/200] [Batch 113/600] [D loss: 0.694656] [G loss: 0.690601]\n",
      "[Epoch 0/200] [Batch 114/600] [D loss: 0.694254] [G loss: 0.691668]\n",
      "[Epoch 0/200] [Batch 115/600] [D loss: 0.693866] [G loss: 0.691588]\n",
      "[Epoch 0/200] [Batch 116/600] [D loss: 0.693193] [G loss: 0.692185]\n",
      "[Epoch 0/200] [Batch 117/600] [D loss: 0.692805] [G loss: 0.692716]\n",
      "[Epoch 0/200] [Batch 118/600] [D loss: 0.692567] [G loss: 0.693182]\n",
      "[Epoch 0/200] [Batch 119/600] [D loss: 0.693323] [G loss: 0.691961]\n",
      "[Epoch 0/200] [Batch 120/600] [D loss: 0.692558] [G loss: 0.692501]\n",
      "[Epoch 0/200] [Batch 121/600] [D loss: 0.691345] [G loss: 0.695524]\n",
      "[Epoch 0/200] [Batch 122/600] [D loss: 0.691509] [G loss: 0.694351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 123/600] [D loss: 0.690201] [G loss: 0.696524]\n",
      "[Epoch 0/200] [Batch 124/600] [D loss: 0.690800] [G loss: 0.696240]\n",
      "[Epoch 0/200] [Batch 125/600] [D loss: 0.690434] [G loss: 0.696700]\n",
      "[Epoch 0/200] [Batch 126/600] [D loss: 0.691258] [G loss: 0.694528]\n",
      "[Epoch 0/200] [Batch 127/600] [D loss: 0.694313] [G loss: 0.688321]\n",
      "[Epoch 0/200] [Batch 128/600] [D loss: 0.697947] [G loss: 0.682950]\n",
      "[Epoch 0/200] [Batch 129/600] [D loss: 0.696964] [G loss: 0.683029]\n",
      "[Epoch 0/200] [Batch 130/600] [D loss: 0.695282] [G loss: 0.687756]\n",
      "[Epoch 0/200] [Batch 131/600] [D loss: 0.695567] [G loss: 0.688036]\n",
      "[Epoch 0/200] [Batch 132/600] [D loss: 0.694076] [G loss: 0.690586]\n",
      "[Epoch 0/200] [Batch 133/600] [D loss: 0.692070] [G loss: 0.693875]\n",
      "[Epoch 0/200] [Batch 134/600] [D loss: 0.689744] [G loss: 0.698524]\n",
      "[Epoch 0/200] [Batch 135/600] [D loss: 0.688573] [G loss: 0.699889]\n",
      "[Epoch 0/200] [Batch 136/600] [D loss: 0.685988] [G loss: 0.702114]\n",
      "[Epoch 0/200] [Batch 137/600] [D loss: 0.684896] [G loss: 0.703263]\n",
      "[Epoch 0/200] [Batch 138/600] [D loss: 0.688106] [G loss: 0.695416]\n",
      "[Epoch 0/200] [Batch 139/600] [D loss: 0.690640] [G loss: 0.689150]\n",
      "[Epoch 0/200] [Batch 140/600] [D loss: 0.692240] [G loss: 0.682702]\n",
      "[Epoch 0/200] [Batch 141/600] [D loss: 0.692111] [G loss: 0.681862]\n",
      "[Epoch 0/200] [Batch 142/600] [D loss: 0.689947] [G loss: 0.691737]\n",
      "[Epoch 0/200] [Batch 143/600] [D loss: 0.696365] [G loss: 0.683750]\n",
      "[Epoch 0/200] [Batch 144/600] [D loss: 0.690997] [G loss: 0.691778]\n",
      "[Epoch 0/200] [Batch 145/600] [D loss: 0.690965] [G loss: 0.693942]\n",
      "[Epoch 0/200] [Batch 146/600] [D loss: 0.688291] [G loss: 0.697905]\n",
      "[Epoch 0/200] [Batch 147/600] [D loss: 0.685997] [G loss: 0.708880]\n",
      "[Epoch 0/200] [Batch 148/600] [D loss: 0.681493] [G loss: 0.719440]\n",
      "[Epoch 0/200] [Batch 149/600] [D loss: 0.679902] [G loss: 0.724696]\n",
      "[Epoch 0/200] [Batch 150/600] [D loss: 0.684538] [G loss: 0.707423]\n",
      "[Epoch 0/200] [Batch 151/600] [D loss: 0.688429] [G loss: 0.701258]\n",
      "[Epoch 0/200] [Batch 152/600] [D loss: 0.692850] [G loss: 0.687620]\n",
      "[Epoch 0/200] [Batch 153/600] [D loss: 0.697173] [G loss: 0.683280]\n",
      "[Epoch 0/200] [Batch 154/600] [D loss: 0.693218] [G loss: 0.683623]\n",
      "[Epoch 0/200] [Batch 155/600] [D loss: 0.696668] [G loss: 0.674959]\n",
      "[Epoch 0/200] [Batch 156/600] [D loss: 0.698415] [G loss: 0.672532]\n",
      "[Epoch 0/200] [Batch 157/600] [D loss: 0.696879] [G loss: 0.675644]\n",
      "[Epoch 0/200] [Batch 158/600] [D loss: 0.694793] [G loss: 0.680377]\n",
      "[Epoch 0/200] [Batch 159/600] [D loss: 0.693368] [G loss: 0.683784]\n",
      "[Epoch 0/200] [Batch 160/600] [D loss: 0.690641] [G loss: 0.689647]\n",
      "[Epoch 0/200] [Batch 161/600] [D loss: 0.690732] [G loss: 0.688086]\n",
      "[Epoch 0/200] [Batch 162/600] [D loss: 0.689813] [G loss: 0.689867]\n",
      "[Epoch 0/200] [Batch 163/600] [D loss: 0.689685] [G loss: 0.688193]\n",
      "[Epoch 0/200] [Batch 164/600] [D loss: 0.685898] [G loss: 0.695576]\n",
      "[Epoch 0/200] [Batch 165/600] [D loss: 0.689381] [G loss: 0.687371]\n",
      "[Epoch 0/200] [Batch 166/600] [D loss: 0.691977] [G loss: 0.685185]\n",
      "[Epoch 0/200] [Batch 167/600] [D loss: 0.689857] [G loss: 0.687425]\n",
      "[Epoch 0/200] [Batch 168/600] [D loss: 0.690015] [G loss: 0.687020]\n",
      "[Epoch 0/200] [Batch 169/600] [D loss: 0.691614] [G loss: 0.682952]\n",
      "[Epoch 0/200] [Batch 170/600] [D loss: 0.692387] [G loss: 0.680021]\n",
      "[Epoch 0/200] [Batch 171/600] [D loss: 0.691897] [G loss: 0.684697]\n",
      "[Epoch 0/200] [Batch 172/600] [D loss: 0.690078] [G loss: 0.686471]\n",
      "[Epoch 0/200] [Batch 173/600] [D loss: 0.690401] [G loss: 0.689501]\n",
      "[Epoch 0/200] [Batch 174/600] [D loss: 0.691303] [G loss: 0.690093]\n",
      "[Epoch 0/200] [Batch 175/600] [D loss: 0.689363] [G loss: 0.699516]\n",
      "[Epoch 0/200] [Batch 176/600] [D loss: 0.685522] [G loss: 0.703898]\n",
      "[Epoch 0/200] [Batch 177/600] [D loss: 0.687648] [G loss: 0.704176]\n",
      "[Epoch 0/200] [Batch 178/600] [D loss: 0.690630] [G loss: 0.699193]\n",
      "[Epoch 0/200] [Batch 179/600] [D loss: 0.691712] [G loss: 0.699102]\n",
      "[Epoch 0/200] [Batch 180/600] [D loss: 0.694401] [G loss: 0.690111]\n",
      "[Epoch 0/200] [Batch 181/600] [D loss: 0.694648] [G loss: 0.692233]\n",
      "[Epoch 0/200] [Batch 182/600] [D loss: 0.695466] [G loss: 0.692453]\n",
      "[Epoch 0/200] [Batch 183/600] [D loss: 0.693083] [G loss: 0.693321]\n",
      "[Epoch 0/200] [Batch 184/600] [D loss: 0.691538] [G loss: 0.696791]\n",
      "[Epoch 0/200] [Batch 185/600] [D loss: 0.690303] [G loss: 0.699740]\n",
      "[Epoch 0/200] [Batch 186/600] [D loss: 0.688520] [G loss: 0.705149]\n",
      "[Epoch 0/200] [Batch 187/600] [D loss: 0.688367] [G loss: 0.707581]\n",
      "[Epoch 0/200] [Batch 188/600] [D loss: 0.687869] [G loss: 0.704232]\n",
      "[Epoch 0/200] [Batch 189/600] [D loss: 0.687057] [G loss: 0.706184]\n",
      "[Epoch 0/200] [Batch 190/600] [D loss: 0.689433] [G loss: 0.700904]\n",
      "[Epoch 0/200] [Batch 191/600] [D loss: 0.691713] [G loss: 0.696305]\n",
      "[Epoch 0/200] [Batch 192/600] [D loss: 0.694626] [G loss: 0.688500]\n",
      "[Epoch 0/200] [Batch 193/600] [D loss: 0.696159] [G loss: 0.685750]\n",
      "[Epoch 0/200] [Batch 194/600] [D loss: 0.697494] [G loss: 0.681974]\n",
      "[Epoch 0/200] [Batch 195/600] [D loss: 0.697953] [G loss: 0.679419]\n",
      "[Epoch 0/200] [Batch 196/600] [D loss: 0.696836] [G loss: 0.681442]\n",
      "[Epoch 0/200] [Batch 197/600] [D loss: 0.696314] [G loss: 0.681904]\n",
      "[Epoch 0/200] [Batch 198/600] [D loss: 0.696299] [G loss: 0.681662]\n",
      "[Epoch 0/200] [Batch 199/600] [D loss: 0.696287] [G loss: 0.683522]\n",
      "[Epoch 0/200] [Batch 200/600] [D loss: 0.694386] [G loss: 0.686202]\n",
      "[Epoch 0/200] [Batch 201/600] [D loss: 0.695139] [G loss: 0.689129]\n",
      "[Epoch 0/200] [Batch 202/600] [D loss: 0.692780] [G loss: 0.691650]\n",
      "[Epoch 0/200] [Batch 203/600] [D loss: 0.692667] [G loss: 0.692909]\n",
      "[Epoch 0/200] [Batch 204/600] [D loss: 0.691139] [G loss: 0.694996]\n",
      "[Epoch 0/200] [Batch 205/600] [D loss: 0.691893] [G loss: 0.694531]\n",
      "[Epoch 0/200] [Batch 206/600] [D loss: 0.691693] [G loss: 0.693950]\n",
      "[Epoch 0/200] [Batch 207/600] [D loss: 0.691459] [G loss: 0.695452]\n",
      "[Epoch 0/200] [Batch 208/600] [D loss: 0.691989] [G loss: 0.693824]\n",
      "[Epoch 0/200] [Batch 209/600] [D loss: 0.691303] [G loss: 0.694980]\n",
      "[Epoch 0/200] [Batch 210/600] [D loss: 0.692186] [G loss: 0.692363]\n",
      "[Epoch 0/200] [Batch 211/600] [D loss: 0.692870] [G loss: 0.692906]\n",
      "[Epoch 0/200] [Batch 212/600] [D loss: 0.693980] [G loss: 0.692409]\n",
      "[Epoch 0/200] [Batch 213/600] [D loss: 0.694114] [G loss: 0.692540]\n",
      "[Epoch 0/200] [Batch 214/600] [D loss: 0.693710] [G loss: 0.693247]\n",
      "[Epoch 0/200] [Batch 215/600] [D loss: 0.693588] [G loss: 0.693672]\n",
      "[Epoch 0/200] [Batch 216/600] [D loss: 0.693537] [G loss: 0.694347]\n",
      "[Epoch 0/200] [Batch 217/600] [D loss: 0.693270] [G loss: 0.695008]\n",
      "[Epoch 0/200] [Batch 218/600] [D loss: 0.693351] [G loss: 0.694551]\n",
      "[Epoch 0/200] [Batch 219/600] [D loss: 0.693734] [G loss: 0.694805]\n",
      "[Epoch 0/200] [Batch 220/600] [D loss: 0.693974] [G loss: 0.695567]\n",
      "[Epoch 0/200] [Batch 221/600] [D loss: 0.694441] [G loss: 0.695963]\n",
      "[Epoch 0/200] [Batch 222/600] [D loss: 0.694035] [G loss: 0.695207]\n",
      "[Epoch 0/200] [Batch 223/600] [D loss: 0.693585] [G loss: 0.695633]\n",
      "[Epoch 0/200] [Batch 224/600] [D loss: 0.693764] [G loss: 0.695858]\n",
      "[Epoch 0/200] [Batch 225/600] [D loss: 0.694042] [G loss: 0.695770]\n",
      "[Epoch 0/200] [Batch 226/600] [D loss: 0.693876] [G loss: 0.697090]\n",
      "[Epoch 0/200] [Batch 227/600] [D loss: 0.694536] [G loss: 0.695934]\n",
      "[Epoch 0/200] [Batch 228/600] [D loss: 0.694250] [G loss: 0.696108]\n",
      "[Epoch 0/200] [Batch 229/600] [D loss: 0.694119] [G loss: 0.695043]\n",
      "[Epoch 0/200] [Batch 230/600] [D loss: 0.694246] [G loss: 0.694922]\n",
      "[Epoch 0/200] [Batch 231/600] [D loss: 0.694121] [G loss: 0.695319]\n",
      "[Epoch 0/200] [Batch 232/600] [D loss: 0.694128] [G loss: 0.695442]\n",
      "[Epoch 0/200] [Batch 233/600] [D loss: 0.694481] [G loss: 0.694936]\n",
      "[Epoch 0/200] [Batch 234/600] [D loss: 0.694402] [G loss: 0.694961]\n",
      "[Epoch 0/200] [Batch 235/600] [D loss: 0.694762] [G loss: 0.694843]\n",
      "[Epoch 0/200] [Batch 236/600] [D loss: 0.694160] [G loss: 0.695377]\n",
      "[Epoch 0/200] [Batch 237/600] [D loss: 0.694910] [G loss: 0.694975]\n",
      "[Epoch 0/200] [Batch 238/600] [D loss: 0.694649] [G loss: 0.695334]\n",
      "[Epoch 0/200] [Batch 239/600] [D loss: 0.694499] [G loss: 0.696508]\n",
      "[Epoch 0/200] [Batch 240/600] [D loss: 0.694463] [G loss: 0.696240]\n",
      "[Epoch 0/200] [Batch 241/600] [D loss: 0.693984] [G loss: 0.695286]\n",
      "[Epoch 0/200] [Batch 242/600] [D loss: 0.694249] [G loss: 0.696139]\n",
      "[Epoch 0/200] [Batch 243/600] [D loss: 0.694105] [G loss: 0.694991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 244/600] [D loss: 0.694144] [G loss: 0.696132]\n",
      "[Epoch 0/200] [Batch 245/600] [D loss: 0.694062] [G loss: 0.694887]\n",
      "[Epoch 0/200] [Batch 246/600] [D loss: 0.693609] [G loss: 0.695582]\n",
      "[Epoch 0/200] [Batch 247/600] [D loss: 0.693411] [G loss: 0.695001]\n",
      "[Epoch 0/200] [Batch 248/600] [D loss: 0.693907] [G loss: 0.694418]\n",
      "[Epoch 0/200] [Batch 249/600] [D loss: 0.693361] [G loss: 0.694307]\n",
      "[Epoch 0/200] [Batch 250/600] [D loss: 0.693810] [G loss: 0.694659]\n",
      "[Epoch 0/200] [Batch 251/600] [D loss: 0.693746] [G loss: 0.694580]\n",
      "[Epoch 0/200] [Batch 252/600] [D loss: 0.693804] [G loss: 0.694092]\n",
      "[Epoch 0/200] [Batch 253/600] [D loss: 0.693289] [G loss: 0.694054]\n",
      "[Epoch 0/200] [Batch 254/600] [D loss: 0.693102] [G loss: 0.693624]\n",
      "[Epoch 0/200] [Batch 255/600] [D loss: 0.693521] [G loss: 0.694337]\n",
      "[Epoch 0/200] [Batch 256/600] [D loss: 0.693449] [G loss: 0.694050]\n",
      "[Epoch 0/200] [Batch 257/600] [D loss: 0.693408] [G loss: 0.694574]\n",
      "[Epoch 0/200] [Batch 258/600] [D loss: 0.693604] [G loss: 0.694272]\n",
      "[Epoch 0/200] [Batch 259/600] [D loss: 0.693344] [G loss: 0.694070]\n",
      "[Epoch 0/200] [Batch 260/600] [D loss: 0.693077] [G loss: 0.694067]\n",
      "[Epoch 0/200] [Batch 261/600] [D loss: 0.693430] [G loss: 0.694093]\n",
      "[Epoch 0/200] [Batch 262/600] [D loss: 0.693186] [G loss: 0.693984]\n",
      "[Epoch 0/200] [Batch 263/600] [D loss: 0.693033] [G loss: 0.693649]\n",
      "[Epoch 0/200] [Batch 264/600] [D loss: 0.693343] [G loss: 0.693855]\n",
      "[Epoch 0/200] [Batch 265/600] [D loss: 0.692793] [G loss: 0.694039]\n",
      "[Epoch 0/200] [Batch 266/600] [D loss: 0.692613] [G loss: 0.694237]\n",
      "[Epoch 0/200] [Batch 267/600] [D loss: 0.693166] [G loss: 0.693837]\n",
      "[Epoch 0/200] [Batch 268/600] [D loss: 0.693069] [G loss: 0.693624]\n",
      "[Epoch 0/200] [Batch 269/600] [D loss: 0.692949] [G loss: 0.693365]\n",
      "[Epoch 0/200] [Batch 270/600] [D loss: 0.693301] [G loss: 0.692998]\n",
      "[Epoch 0/200] [Batch 271/600] [D loss: 0.693316] [G loss: 0.693232]\n",
      "[Epoch 0/200] [Batch 272/600] [D loss: 0.693266] [G loss: 0.693198]\n",
      "[Epoch 0/200] [Batch 273/600] [D loss: 0.692947] [G loss: 0.694277]\n",
      "[Epoch 0/200] [Batch 274/600] [D loss: 0.693145] [G loss: 0.694283]\n",
      "[Epoch 0/200] [Batch 275/600] [D loss: 0.692737] [G loss: 0.694446]\n",
      "[Epoch 0/200] [Batch 276/600] [D loss: 0.692843] [G loss: 0.694928]\n",
      "[Epoch 0/200] [Batch 277/600] [D loss: 0.692765] [G loss: 0.694953]\n",
      "[Epoch 0/200] [Batch 278/600] [D loss: 0.692708] [G loss: 0.694379]\n",
      "[Epoch 0/200] [Batch 279/600] [D loss: 0.692254] [G loss: 0.694697]\n",
      "[Epoch 0/200] [Batch 280/600] [D loss: 0.692720] [G loss: 0.694130]\n",
      "[Epoch 0/200] [Batch 281/600] [D loss: 0.692544] [G loss: 0.694027]\n",
      "[Epoch 0/200] [Batch 282/600] [D loss: 0.693091] [G loss: 0.694252]\n",
      "[Epoch 0/200] [Batch 283/600] [D loss: 0.692449] [G loss: 0.695283]\n",
      "[Epoch 0/200] [Batch 284/600] [D loss: 0.692449] [G loss: 0.695252]\n",
      "[Epoch 0/200] [Batch 285/600] [D loss: 0.692386] [G loss: 0.694595]\n",
      "[Epoch 0/200] [Batch 286/600] [D loss: 0.692717] [G loss: 0.694837]\n",
      "[Epoch 0/200] [Batch 287/600] [D loss: 0.692875] [G loss: 0.694664]\n",
      "[Epoch 0/200] [Batch 288/600] [D loss: 0.692599] [G loss: 0.694413]\n",
      "[Epoch 0/200] [Batch 289/600] [D loss: 0.692589] [G loss: 0.694630]\n",
      "[Epoch 0/200] [Batch 290/600] [D loss: 0.692122] [G loss: 0.694161]\n",
      "[Epoch 0/200] [Batch 291/600] [D loss: 0.692497] [G loss: 0.693820]\n",
      "[Epoch 0/200] [Batch 292/600] [D loss: 0.692676] [G loss: 0.693502]\n",
      "[Epoch 0/200] [Batch 293/600] [D loss: 0.692310] [G loss: 0.693841]\n",
      "[Epoch 0/200] [Batch 294/600] [D loss: 0.692594] [G loss: 0.694210]\n",
      "[Epoch 0/200] [Batch 295/600] [D loss: 0.692745] [G loss: 0.693663]\n",
      "[Epoch 0/200] [Batch 296/600] [D loss: 0.692514] [G loss: 0.694100]\n",
      "[Epoch 0/200] [Batch 297/600] [D loss: 0.692715] [G loss: 0.693358]\n",
      "[Epoch 0/200] [Batch 298/600] [D loss: 0.692020] [G loss: 0.693902]\n",
      "[Epoch 0/200] [Batch 299/600] [D loss: 0.692675] [G loss: 0.693607]\n",
      "[Epoch 0/200] [Batch 300/600] [D loss: 0.692483] [G loss: 0.693878]\n",
      "[Epoch 0/200] [Batch 301/600] [D loss: 0.692426] [G loss: 0.693361]\n",
      "[Epoch 0/200] [Batch 302/600] [D loss: 0.692470] [G loss: 0.693601]\n",
      "[Epoch 0/200] [Batch 303/600] [D loss: 0.692896] [G loss: 0.693454]\n",
      "[Epoch 0/200] [Batch 304/600] [D loss: 0.692321] [G loss: 0.693193]\n",
      "[Epoch 0/200] [Batch 305/600] [D loss: 0.691724] [G loss: 0.693532]\n",
      "[Epoch 0/200] [Batch 306/600] [D loss: 0.692556] [G loss: 0.693116]\n",
      "[Epoch 0/200] [Batch 307/600] [D loss: 0.692651] [G loss: 0.692441]\n",
      "[Epoch 0/200] [Batch 308/600] [D loss: 0.693149] [G loss: 0.692895]\n",
      "[Epoch 0/200] [Batch 309/600] [D loss: 0.692549] [G loss: 0.692441]\n",
      "[Epoch 0/200] [Batch 310/600] [D loss: 0.692787] [G loss: 0.693509]\n",
      "[Epoch 0/200] [Batch 311/600] [D loss: 0.692487] [G loss: 0.694488]\n",
      "[Epoch 0/200] [Batch 312/600] [D loss: 0.692997] [G loss: 0.694151]\n",
      "[Epoch 0/200] [Batch 313/600] [D loss: 0.692147] [G loss: 0.693826]\n",
      "[Epoch 0/200] [Batch 314/600] [D loss: 0.692751] [G loss: 0.693696]\n",
      "[Epoch 0/200] [Batch 315/600] [D loss: 0.692770] [G loss: 0.693946]\n",
      "[Epoch 0/200] [Batch 316/600] [D loss: 0.692956] [G loss: 0.693567]\n",
      "[Epoch 0/200] [Batch 317/600] [D loss: 0.693037] [G loss: 0.693392]\n",
      "[Epoch 0/200] [Batch 318/600] [D loss: 0.692767] [G loss: 0.694172]\n",
      "[Epoch 0/200] [Batch 319/600] [D loss: 0.692455] [G loss: 0.693865]\n",
      "[Epoch 0/200] [Batch 320/600] [D loss: 0.692609] [G loss: 0.693215]\n",
      "[Epoch 0/200] [Batch 321/600] [D loss: 0.692358] [G loss: 0.695244]\n",
      "[Epoch 0/200] [Batch 322/600] [D loss: 0.691852] [G loss: 0.693862]\n",
      "[Epoch 0/200] [Batch 323/600] [D loss: 0.691626] [G loss: 0.694607]\n",
      "[Epoch 0/200] [Batch 324/600] [D loss: 0.691766] [G loss: 0.694061]\n",
      "[Epoch 0/200] [Batch 325/600] [D loss: 0.691472] [G loss: 0.694222]\n",
      "[Epoch 0/200] [Batch 326/600] [D loss: 0.691088] [G loss: 0.693972]\n",
      "[Epoch 0/200] [Batch 327/600] [D loss: 0.691275] [G loss: 0.694161]\n",
      "[Epoch 0/200] [Batch 328/600] [D loss: 0.690563] [G loss: 0.694366]\n",
      "[Epoch 0/200] [Batch 329/600] [D loss: 0.691594] [G loss: 0.693662]\n",
      "[Epoch 0/200] [Batch 330/600] [D loss: 0.691525] [G loss: 0.693262]\n",
      "[Epoch 0/200] [Batch 331/600] [D loss: 0.691687] [G loss: 0.693315]\n",
      "[Epoch 0/200] [Batch 332/600] [D loss: 0.691508] [G loss: 0.692549]\n",
      "[Epoch 0/200] [Batch 333/600] [D loss: 0.692001] [G loss: 0.691939]\n",
      "[Epoch 0/200] [Batch 334/600] [D loss: 0.692822] [G loss: 0.691175]\n",
      "[Epoch 0/200] [Batch 335/600] [D loss: 0.691596] [G loss: 0.691733]\n",
      "[Epoch 0/200] [Batch 336/600] [D loss: 0.691529] [G loss: 0.691625]\n",
      "[Epoch 0/200] [Batch 337/600] [D loss: 0.691688] [G loss: 0.690806]\n",
      "[Epoch 0/200] [Batch 338/600] [D loss: 0.692143] [G loss: 0.693255]\n",
      "[Epoch 0/200] [Batch 339/600] [D loss: 0.690503] [G loss: 0.693543]\n",
      "[Epoch 0/200] [Batch 340/600] [D loss: 0.690131] [G loss: 0.694084]\n",
      "[Epoch 0/200] [Batch 341/600] [D loss: 0.691051] [G loss: 0.693329]\n",
      "[Epoch 0/200] [Batch 342/600] [D loss: 0.691123] [G loss: 0.692332]\n",
      "[Epoch 0/200] [Batch 343/600] [D loss: 0.691523] [G loss: 0.693070]\n",
      "[Epoch 0/200] [Batch 344/600] [D loss: 0.691503] [G loss: 0.691385]\n",
      "[Epoch 0/200] [Batch 345/600] [D loss: 0.691691] [G loss: 0.691659]\n",
      "[Epoch 0/200] [Batch 346/600] [D loss: 0.690242] [G loss: 0.692812]\n",
      "[Epoch 0/200] [Batch 347/600] [D loss: 0.692954] [G loss: 0.688872]\n",
      "[Epoch 0/200] [Batch 348/600] [D loss: 0.692557] [G loss: 0.689951]\n",
      "[Epoch 0/200] [Batch 349/600] [D loss: 0.692566] [G loss: 0.692476]\n",
      "[Epoch 0/200] [Batch 350/600] [D loss: 0.690919] [G loss: 0.693593]\n",
      "[Epoch 0/200] [Batch 351/600] [D loss: 0.689662] [G loss: 0.695672]\n",
      "[Epoch 0/200] [Batch 352/600] [D loss: 0.688400] [G loss: 0.695384]\n",
      "[Epoch 0/200] [Batch 353/600] [D loss: 0.689952] [G loss: 0.697422]\n",
      "[Epoch 0/200] [Batch 354/600] [D loss: 0.689527] [G loss: 0.697642]\n",
      "[Epoch 0/200] [Batch 355/600] [D loss: 0.692073] [G loss: 0.693831]\n",
      "[Epoch 0/200] [Batch 356/600] [D loss: 0.688753] [G loss: 0.697024]\n",
      "[Epoch 0/200] [Batch 357/600] [D loss: 0.691998] [G loss: 0.693543]\n",
      "[Epoch 0/200] [Batch 358/600] [D loss: 0.694629] [G loss: 0.684432]\n",
      "[Epoch 0/200] [Batch 359/600] [D loss: 0.694955] [G loss: 0.686712]\n",
      "[Epoch 0/200] [Batch 360/600] [D loss: 0.694969] [G loss: 0.689590]\n",
      "[Epoch 0/200] [Batch 361/600] [D loss: 0.694396] [G loss: 0.690016]\n",
      "[Epoch 0/200] [Batch 362/600] [D loss: 0.692264] [G loss: 0.693739]\n",
      "[Epoch 0/200] [Batch 363/600] [D loss: 0.691812] [G loss: 0.695575]\n",
      "[Epoch 0/200] [Batch 364/600] [D loss: 0.687951] [G loss: 0.702912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 365/600] [D loss: 0.688662] [G loss: 0.703621]\n",
      "[Epoch 0/200] [Batch 366/600] [D loss: 0.687236] [G loss: 0.705937]\n",
      "[Epoch 0/200] [Batch 367/600] [D loss: 0.687772] [G loss: 0.701269]\n",
      "[Epoch 0/200] [Batch 368/600] [D loss: 0.688499] [G loss: 0.700335]\n",
      "[Epoch 0/200] [Batch 369/600] [D loss: 0.693685] [G loss: 0.693836]\n",
      "[Epoch 0/200] [Batch 370/600] [D loss: 0.691179] [G loss: 0.693628]\n",
      "[Epoch 0/200] [Batch 371/600] [D loss: 0.695910] [G loss: 0.688107]\n",
      "[Epoch 0/200] [Batch 372/600] [D loss: 0.694172] [G loss: 0.686754]\n",
      "[Epoch 0/200] [Batch 373/600] [D loss: 0.694417] [G loss: 0.687345]\n",
      "[Epoch 0/200] [Batch 374/600] [D loss: 0.691179] [G loss: 0.688045]\n",
      "[Epoch 0/200] [Batch 375/600] [D loss: 0.690243] [G loss: 0.693321]\n",
      "[Epoch 0/200] [Batch 376/600] [D loss: 0.688634] [G loss: 0.694319]\n",
      "[Epoch 0/200] [Batch 377/600] [D loss: 0.685554] [G loss: 0.697927]\n",
      "[Epoch 0/200] [Batch 378/600] [D loss: 0.687112] [G loss: 0.696127]\n",
      "[Epoch 0/200] [Batch 379/600] [D loss: 0.691515] [G loss: 0.695665]\n",
      "[Epoch 0/200] [Batch 380/600] [D loss: 0.693235] [G loss: 0.686764]\n",
      "[Epoch 0/200] [Batch 381/600] [D loss: 0.692121] [G loss: 0.691506]\n",
      "[Epoch 0/200] [Batch 382/600] [D loss: 0.695182] [G loss: 0.686064]\n",
      "[Epoch 0/200] [Batch 383/600] [D loss: 0.695688] [G loss: 0.690323]\n",
      "[Epoch 0/200] [Batch 384/600] [D loss: 0.692719] [G loss: 0.690429]\n",
      "[Epoch 0/200] [Batch 385/600] [D loss: 0.690342] [G loss: 0.695429]\n",
      "[Epoch 0/200] [Batch 386/600] [D loss: 0.689065] [G loss: 0.698420]\n",
      "[Epoch 0/200] [Batch 387/600] [D loss: 0.687485] [G loss: 0.698809]\n",
      "[Epoch 0/200] [Batch 388/600] [D loss: 0.688492] [G loss: 0.700335]\n",
      "[Epoch 0/200] [Batch 389/600] [D loss: 0.688382] [G loss: 0.701459]\n",
      "[Epoch 0/200] [Batch 390/600] [D loss: 0.690178] [G loss: 0.696482]\n",
      "[Epoch 0/200] [Batch 391/600] [D loss: 0.692712] [G loss: 0.692736]\n",
      "[Epoch 0/200] [Batch 392/600] [D loss: 0.695266] [G loss: 0.690871]\n",
      "[Epoch 0/200] [Batch 393/600] [D loss: 0.694094] [G loss: 0.694248]\n",
      "[Epoch 0/200] [Batch 394/600] [D loss: 0.692106] [G loss: 0.697387]\n",
      "[Epoch 0/200] [Batch 395/600] [D loss: 0.693188] [G loss: 0.692681]\n",
      "[Epoch 0/200] [Batch 396/600] [D loss: 0.692265] [G loss: 0.694362]\n",
      "[Epoch 0/200] [Batch 397/600] [D loss: 0.693406] [G loss: 0.693398]\n",
      "[Epoch 0/200] [Batch 398/600] [D loss: 0.691254] [G loss: 0.696251]\n",
      "[Epoch 0/200] [Batch 399/600] [D loss: 0.688929] [G loss: 0.696194]\n",
      "[Epoch 0/200] [Batch 400/600] [D loss: 0.689672] [G loss: 0.697513]\n",
      "[Epoch 0/200] [Batch 401/600] [D loss: 0.688201] [G loss: 0.693147]\n",
      "[Epoch 0/200] [Batch 402/600] [D loss: 0.688677] [G loss: 0.695190]\n",
      "[Epoch 0/200] [Batch 403/600] [D loss: 0.688014] [G loss: 0.693531]\n",
      "[Epoch 0/200] [Batch 404/600] [D loss: 0.692448] [G loss: 0.687208]\n",
      "[Epoch 0/200] [Batch 405/600] [D loss: 0.694979] [G loss: 0.684699]\n",
      "[Epoch 0/200] [Batch 406/600] [D loss: 0.694191] [G loss: 0.681456]\n",
      "[Epoch 0/200] [Batch 407/600] [D loss: 0.694843] [G loss: 0.687918]\n",
      "[Epoch 0/200] [Batch 408/600] [D loss: 0.696513] [G loss: 0.690000]\n",
      "[Epoch 0/200] [Batch 409/600] [D loss: 0.694176] [G loss: 0.694530]\n",
      "[Epoch 0/200] [Batch 410/600] [D loss: 0.690983] [G loss: 0.694797]\n",
      "[Epoch 0/200] [Batch 411/600] [D loss: 0.690327] [G loss: 0.699167]\n",
      "[Epoch 0/200] [Batch 412/600] [D loss: 0.690168] [G loss: 0.698130]\n",
      "[Epoch 0/200] [Batch 413/600] [D loss: 0.688275] [G loss: 0.701788]\n",
      "[Epoch 0/200] [Batch 414/600] [D loss: 0.686867] [G loss: 0.697550]\n",
      "[Epoch 0/200] [Batch 415/600] [D loss: 0.686985] [G loss: 0.700579]\n",
      "[Epoch 0/200] [Batch 416/600] [D loss: 0.687804] [G loss: 0.698313]\n",
      "[Epoch 0/200] [Batch 417/600] [D loss: 0.692816] [G loss: 0.695446]\n",
      "[Epoch 0/200] [Batch 418/600] [D loss: 0.693444] [G loss: 0.696991]\n",
      "[Epoch 0/200] [Batch 419/600] [D loss: 0.692371] [G loss: 0.696226]\n",
      "[Epoch 0/200] [Batch 420/600] [D loss: 0.693773] [G loss: 0.690121]\n",
      "[Epoch 0/200] [Batch 421/600] [D loss: 0.692729] [G loss: 0.688536]\n",
      "[Epoch 0/200] [Batch 422/600] [D loss: 0.696380] [G loss: 0.688030]\n",
      "[Epoch 0/200] [Batch 423/600] [D loss: 0.692526] [G loss: 0.686205]\n",
      "[Epoch 0/200] [Batch 424/600] [D loss: 0.692979] [G loss: 0.690941]\n",
      "[Epoch 0/200] [Batch 425/600] [D loss: 0.692995] [G loss: 0.695519]\n",
      "[Epoch 0/200] [Batch 426/600] [D loss: 0.688930] [G loss: 0.698307]\n",
      "[Epoch 0/200] [Batch 427/600] [D loss: 0.689561] [G loss: 0.704604]\n",
      "[Epoch 0/200] [Batch 428/600] [D loss: 0.687270] [G loss: 0.710477]\n",
      "[Epoch 0/200] [Batch 429/600] [D loss: 0.689203] [G loss: 0.708772]\n",
      "[Epoch 0/200] [Batch 430/600] [D loss: 0.685520] [G loss: 0.710505]\n",
      "[Epoch 0/200] [Batch 431/600] [D loss: 0.687909] [G loss: 0.705490]\n",
      "[Epoch 0/200] [Batch 432/600] [D loss: 0.687275] [G loss: 0.701910]\n",
      "[Epoch 0/200] [Batch 433/600] [D loss: 0.689603] [G loss: 0.698925]\n",
      "[Epoch 0/200] [Batch 434/600] [D loss: 0.691088] [G loss: 0.686611]\n",
      "[Epoch 0/200] [Batch 435/600] [D loss: 0.698972] [G loss: 0.677031]\n",
      "[Epoch 0/200] [Batch 436/600] [D loss: 0.695954] [G loss: 0.673933]\n",
      "[Epoch 0/200] [Batch 437/600] [D loss: 0.693825] [G loss: 0.677355]\n",
      "[Epoch 0/200] [Batch 438/600] [D loss: 0.696951] [G loss: 0.678459]\n",
      "[Epoch 0/200] [Batch 439/600] [D loss: 0.696100] [G loss: 0.677372]\n",
      "[Epoch 0/200] [Batch 440/600] [D loss: 0.691514] [G loss: 0.689213]\n",
      "[Epoch 0/200] [Batch 441/600] [D loss: 0.688144] [G loss: 0.698679]\n",
      "[Epoch 0/200] [Batch 442/600] [D loss: 0.686935] [G loss: 0.704383]\n",
      "[Epoch 0/200] [Batch 443/600] [D loss: 0.683867] [G loss: 0.710370]\n",
      "[Epoch 0/200] [Batch 444/600] [D loss: 0.683661] [G loss: 0.711134]\n",
      "[Epoch 0/200] [Batch 445/600] [D loss: 0.683593] [G loss: 0.712421]\n",
      "[Epoch 0/200] [Batch 446/600] [D loss: 0.688592] [G loss: 0.696605]\n",
      "[Epoch 0/200] [Batch 447/600] [D loss: 0.694221] [G loss: 0.678090]\n",
      "[Epoch 0/200] [Batch 448/600] [D loss: 0.688063] [G loss: 0.687661]\n",
      "[Epoch 0/200] [Batch 449/600] [D loss: 0.695156] [G loss: 0.675365]\n",
      "[Epoch 0/200] [Batch 450/600] [D loss: 0.693171] [G loss: 0.688123]\n",
      "[Epoch 0/200] [Batch 451/600] [D loss: 0.697761] [G loss: 0.684090]\n",
      "[Epoch 0/200] [Batch 452/600] [D loss: 0.689028] [G loss: 0.699544]\n",
      "[Epoch 0/200] [Batch 453/600] [D loss: 0.689467] [G loss: 0.712288]\n",
      "[Epoch 0/200] [Batch 454/600] [D loss: 0.686826] [G loss: 0.719863]\n",
      "[Epoch 0/200] [Batch 455/600] [D loss: 0.683833] [G loss: 0.715429]\n",
      "[Epoch 0/200] [Batch 456/600] [D loss: 0.689026] [G loss: 0.706705]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-325-419510f397fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversarial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs,_) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], 100))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        \n",
    "        save_image(gen_imgs.data[:25], \"images-1/%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
